{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazySparkSession:\n",
    "    packages = [\n",
    "        \"io.delta:delta-spark_2.13:4.0.0\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, access_key, secret_key, endpoint):\n",
    "        self._access_key = access_key\n",
    "        self._secret_key = secret_key\n",
    "        self._endpoint = endpoint\n",
    "        \n",
    "\n",
    "    def start(\n",
    "        self,\n",
    "        app_name: str = \"Airflow Spark Delta Minio App\",\n",
    "        executor_memory: str = \"1g\",\n",
    "        driver_memory: str = \"1g\",\n",
    "        driver_maxresultsize: str = \"1g\",\n",
    "        master_url: str = \"local[*]\",\n",
    "    ):\n",
    "\n",
    "        builder = (\n",
    "            SparkSession\n",
    "            .Builder()\n",
    "            .appName(app_name)\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", self._access_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", self._secret_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", self._endpoint)\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "            #\n",
    "            .config(\"spark.hadoop.delta.enableFastS3AListFrom\", \"true\")\n",
    "            #\n",
    "            # .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://127.0.0.1:9083\")\n",
    "            #\n",
    "            .config(\"spark.executor.memory\", executor_memory)\n",
    "            #\n",
    "            .config(\"spark.driver.memory\", driver_memory)\n",
    "            .config(\"spark.driver.maxResultSize\", driver_maxresultsize)\n",
    "            #\n",
    "            # .config(\"spark.sql.warehouse.dir\", \"s3a://lakehouse\")\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            # .config(\"spark.sql.hive.metastore.version\", \"4.0.0\")\n",
    "            # .config(\"spark.sql.hive.metastore.jars\", \"lib/*\")\n",
    "            #\n",
    "            .config(\"spark.jars.packages\", \",\".join(self.packages))\n",
    "            #\n",
    "            .master(master_url)\n",
    "            # .enableHiveSupport()\n",
    "        )\n",
    "\n",
    "        return builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be166f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = LazySparkSession(\n",
    "    access_key=\"9a5edc7df42cfd71\", \n",
    "    secret_key=\"DW4CyCit2CGO9gXyRtqQ7Lj6ZzQZm3xfS33ItBQpVbc=\", \n",
    "    endpoint=\"http://127.0.0.1:10000\"\n",
    ").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1,2), (1,2), (1,2)], schema=\"a integer, b integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12746726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode('overwrite').save(\"s3a://bronze/numbers/integers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format('delta').load('s3a://bronze/numbers/integers').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25/08/21 07:54:12 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
    "# SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
    "# SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
    "# SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
    "\n",
    "# 25/08/21 07:54:15 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
    "# Hive Session ID = e38db77a-6afe-437e-bc8e-d5fa7f632b38\n",
    "# 25/08/21 07:54:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
    "# 25/08/21 07:54:51 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to numbers/integer/part-00000-8c5f40d3-4def-4580-b432-fc63efb209cc-c000.snappy.parquet. This is Unsupported\n",
    "# 25/08/21 07:55:02 WARN HiveExternalCatalog: Could not alter schema of table `numbers`.`integer` in a Hive compatible way. Updating Hive metastore in Spark SQL specific format.\n",
    "\n",
    "# Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:34253\n",
    "\n",
    "# WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32bff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create catalog bronze using delta_lake with (\n",
    "#     \"hive.metastore.uri\"='thrift://host.docker.internal:9083',\n",
    "#     \"s3.aws-access-key\"='9a5edc7df42cfd71',\n",
    "#     \"s3.aws-secret-key\"='DW4CyCit2CGO9gXyRtqQ7Lj6ZzQZm3xfS33ItBQpVbc=',\n",
    "#     \"s3.endpoint\"='http://host.docker.internal:10000',\n",
    "#     \"s3.region\"='us-east-1',\n",
    "#     \"fs.native-s3.enabled\"='true',\n",
    "#     \"s3.path-style-access\"='true',\n",
    "#     \"delta.metastore.store-table-metadata\"='true',\n",
    "#     \"delta.register-table-procedure.enabled\"='true',\n",
    "#     \"delta.deletion-vectors-enabled\"='true',\n",
    "#     \"delta.enable-non-concurrent-writes\"='true'\n",
    "# )\n",
    "\n",
    "# create schema if not exists number with (\n",
    "#     \"location\" = 's3a://bronze/numbers'\n",
    "# )\n",
    "\n",
    "# CALL bronze.system.register_table(schema_name => 'numbers', table_name => 'integers', table_location => 's3://bronze/numbers/integergs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Git Commit Message Convention for Your Team\n",
    "\n",
    "# https://medium.com/@naandalist/creating-a-git-commit-message-convention-for-your-team-acb4b3edfc44"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto-lakehouse (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
