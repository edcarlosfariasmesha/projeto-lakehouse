{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd7da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import zipfile\n",
    "from datetime import date\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "import aiohttp\n",
    "from minio import Minio\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dbf7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_connection = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4dc877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração básica de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd433da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregar para funcionar\n",
    "try:\n",
    "    minio_conn = json.loads(minio_connection)\n",
    "except json.JSONDecodeError:\n",
    "    with open(\"../variables/minio_connection.json\", \"r\") as minio_connection_file:\n",
    "        minio_conn = json.loads(minio_connection_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9e1f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 11:40:52,970 - INFO - Cliente MinIO criado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "s3_client = None\n",
    "\n",
    "try:\n",
    "    endpoint_raw = minio_conn[\"endpoint\"]\n",
    "    access_key = minio_conn[\"access_key\"]\n",
    "    secret_key = minio_conn[\"key\"]\n",
    "\n",
    "    endpoint_sem_http = endpoint_raw.replace(\"http://\", \"\").replace(\"https://\", \"\")\n",
    "    is_secure = endpoint_raw.startswith(\"https\")\n",
    "\n",
    "    s3_client = Minio(\n",
    "        endpoint=endpoint_sem_http,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "        secure=is_secure\n",
    "    )\n",
    "\n",
    "    logging.info(\"Cliente MinIO criado com sucesso.\")\n",
    "\n",
    "except KeyError as e:\n",
    "    logging.error(f\"Erro de configuração: chave ausente - {e}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao inicializar o cliente MinIO: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e618ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/\"\n",
    "bucket = \"landing\"\n",
    "schema = \"rfb\"\n",
    "table = \"cnpj_qualificacoes\"\n",
    "\n",
    "archives = [\n",
    "    \"Qualificacoes.zip\",\n",
    "]\n",
    "\n",
    "current_year = date.today().year\n",
    "current_month = date.today().month\n",
    "\n",
    "years_to_download = 5\n",
    "start_year = current_year - years_to_download + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73a53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função para listar diretórios existentes ---\n",
    "async def listar_diretorios_existentes():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as resp:\n",
    "            html = await resp.text()\n",
    "            return re.findall(r\"(\\d{4}-\\d{2})/\", html)  # pega só YYYY-MM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f75cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(session: aiohttp.ClientSession, url: str, sem: asyncio.Semaphore, retries=3):\n",
    "    async with sem:\n",
    "        for tentativa in range(1, retries+1):\n",
    "            try:\n",
    "                timeout = aiohttp.ClientTimeout(total=600)\n",
    "                async with session.get(url, timeout=timeout) as response:\n",
    "\n",
    "                    status = response.status\n",
    "                    if status != 200:\n",
    "                        logger.error(f\"Url: {url}, Status: {status}, Message: Status inesperado\")\n",
    "                        return None\n",
    "\n",
    "                    content_type = response.headers.get(\"Content-Type\")\n",
    "                    if content_type != \"application/zip\":\n",
    "                        logger.error(f\"Url: {url}, Status: {status}, Message: Tipo inesperado: {content_type}\")\n",
    "                        return None\n",
    "\n",
    "                    content_length = response.headers.get(\"Content-Length\")\n",
    "                    if content_length is None or int(content_length) == 0:\n",
    "                        logger.error(f\"Url: {url}, Status: {status}, Message: Tamanho indefinido ou inesperado\")\n",
    "                        return None\n",
    "\n",
    "                    filename = url.split(\"/\")[-1]\n",
    "                    path_file = f\"download/{filename}\"\n",
    "                    with open(path_file, \"wb\") as zip_file:\n",
    "                        async for chunk in response.content.iter_chunked(1024*1024):\n",
    "                            zip_file.write(chunk)\n",
    "                    logger.info(f\"Baixado: {filename}\")\n",
    "\n",
    "                    # Extrai o zip\n",
    "                    try:\n",
    "                        with zipfile.ZipFile(path_file, \"r\") as zf:\n",
    "                            zf.extractall(\"download\")\n",
    "                        logger.info(f\"Extraído: {filename}\")\n",
    "                    except zipfile.BadZipFile as bz:\n",
    "                        logger.error(f\"Url: {url}, Error: {bz}, Message: Erro ao tentar descompactar o arquivo: {filename}\")\n",
    "                    return  # sucesso\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Tentativa {tentativa}/{retries} falhou para {url}: {e}\")\n",
    "                await asyncio.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bdb9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função para download com limite de concorrência ---\n",
    "async def downloader(urls):\n",
    "    sem = asyncio.Semaphore()  # Limite de concorrência\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch(session, url, sem) for url in urls]\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb74f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 11:40:53,388 - INFO - 28 arquivos para baixar\n",
      "2025-08-18 11:40:53,747 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:53,750 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:53,834 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:53,836 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:53,921 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:53,923 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,009 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,011 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,096 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,098 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,183 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,184 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,269 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,270 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,355 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,356 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,442 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,443 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,527 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,529 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,614 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,616 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,701 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,703 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,788 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,790 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,873 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,875 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,961 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:54,962 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,046 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,048 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,134 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,136 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,221 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,223 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,309 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,311 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,397 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,398 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,483 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,485 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,569 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,571 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,654 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,655 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,741 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,743 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,827 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,830 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,915 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,916 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:55,999 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:56,001 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:56,087 - INFO - Baixado: Qualificacoes.zip\n",
      "2025-08-18 11:40:56,089 - INFO - Extraído: Qualificacoes.zip\n",
      "2025-08-18 11:40:56,091 - INFO - Arquivos de Qualificacoes encontrados: 1\n",
      "2025-08-18 11:40:56,094 - INFO - Total de linhas consolidadas: 67\n"
     ]
    }
   ],
   "source": [
    "# --- Função principal ---\n",
    "async def main():\n",
    "    dirs_existentes = await listar_diretorios_existentes()\n",
    "\n",
    "    urls_para_baixar = []\n",
    "    for year in range(start_year, current_year + 1):\n",
    "        last_month = current_month if year == current_year else 12\n",
    "        for month in range(1, last_month + 1):\n",
    "            dir_name = f\"{year}-{str(month).zfill(2)}\"\n",
    "            if dir_name in dirs_existentes:  # só pega meses que existem\n",
    "                for archive in archives:\n",
    "                    urls_para_baixar.append(f\"{url}{dir_name}/{archive}\")\n",
    "\n",
    "    logger.info(f\"{len(urls_para_baixar)} arquivos para baixar\")\n",
    "    await downloader(urls_para_baixar)\n",
    "\n",
    "    # --- Após baixar, filtra apenas arquivos de Qualificações ---\n",
    "    pasta_base = Path(\"download\")\n",
    "    padrao_arquivo = \"Qualificacoes\"\n",
    "    arquivos_qualificacoes = [arq for arq in pasta_base.rglob(\"*\") if padrao_arquivo.lower() in arq.name.lower()]\n",
    "\n",
    "    logger.info(f\"Arquivos de Qualificacoes encontrados: {len(arquivos_qualificacoes)}\")\n",
    "    dfs = []\n",
    "    for arquivo in arquivos_qualificacoes:\n",
    "        try:\n",
    "            df = pd.read_csv(arquivo, sep=\";\", encoding=\"latin1\", low_memory=False)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao ler {arquivo}: {e}\")\n",
    "\n",
    "    if dfs:\n",
    "        df_final = pd.concat(dfs, ignore_index=True)\n",
    "        logger.info(f\"Total de linhas consolidadas: {len(df_final)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Path(\"download\").mkdir(exist_ok=True)\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c7cf43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_base = Path(\"download\")\n",
    "padrao_arquivo = \"QUALSCSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4164b2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 11:40:56,114 - INFO - Arquivos CSV de qualificacoes encontrados: 28\n"
     ]
    }
   ],
   "source": [
    "# Lista apenas CSVs\n",
    "arquivos_qualificacoes = [arq for arq in pasta_base.rglob(\"*\") if padrao_arquivo in arq.name.upper()]\n",
    "\n",
    "logger.info(f\"Arquivos CSV de qualificacoes encontrados: {len(arquivos_qualificacoes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3681a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload para o MinIO\n",
    "for arquivo in arquivos_qualificacoes:\n",
    "    caminho_relativo = arquivo.relative_to(pasta_base)\n",
    "    destino = f\"rfb/cnpj_qualificacoes/{caminho_relativo.as_posix()}\"\n",
    "    s3_client.fput_object(\n",
    "        bucket_name=\"landing\",\n",
    "        object_name=destino,\n",
    "        file_path=str(arquivo)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc6ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 11:40:56,443 - INFO - Pasta 'download' removida com sucesso após upload.\n"
     ]
    }
   ],
   "source": [
    "# Limpeza da pasta download após upload\n",
    "downloads_path = Path(\"download\")\n",
    "try:\n",
    "    if downloads_path.exists():\n",
    "        shutil.rmtree(downloads_path)\n",
    "        logger.info(f\"Pasta '{downloads_path}' removida com sucesso após upload.\")\n",
    "    else:\n",
    "        logger.warning(f\"Pasta '{downloads_path}' não encontrada para remoção.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao tentar remover '{downloads_path}': {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto-lakehouse (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
