{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd7da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from minio import Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dbf7d0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "minio_connection = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4dc877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração básica de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd433da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregar para funcionar\n",
    "try:\n",
    "    minio_conn = json.loads(minio_connection)\n",
    "except json.JSONDecodeError:\n",
    "    with open(\"../variables/minio_connection.json\", \"r\") as minio_connection_file:\n",
    "        minio_conn = json.loads(minio_connection_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9e1f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 09:55:40,854 - INFO - Cliente MinIO criado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "s3_client = None\n",
    "\n",
    "try:\n",
    "    endpoint_raw = minio_conn[\"endpoint\"]\n",
    "    access_key = minio_conn[\"access_key\"]\n",
    "    secret_key = minio_conn[\"key\"]\n",
    "\n",
    "    endpoint_sem_http = endpoint_raw.replace(\"http://\", \"\").replace(\"https://\", \"\")\n",
    "    is_secure = endpoint_raw.startswith(\"https\")\n",
    "\n",
    "    s3_client = Minio(\n",
    "        endpoint=endpoint_sem_http,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "        secure=is_secure\n",
    "    )\n",
    "\n",
    "    logging.info(\"Cliente MinIO criado com sucesso.\")\n",
    "\n",
    "except KeyError as e:\n",
    "    logging.error(f\"Erro de configuração: chave ausente - {e}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao inicializar o cliente MinIO: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e618ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/\"\n",
    "bucket = \"landing\"\n",
    "schema = \"rfb\"\n",
    "table = \"cnpj_empresas\"\n",
    "\n",
    "archives = [\n",
    "    \"Empresas0.zip\",\n",
    "    \"Empresas1.zip\",\n",
    "    \"Empresas2.zip\",\n",
    "    \"Empresas3.zip\",\n",
    "    \"Empresas4.zip\",\n",
    "    \"Empresas5.zip\",\n",
    "    \"Empresas6.zip\",\n",
    "    \"Empresas7.zip\",\n",
    "    \"Empresas8.zip\",\n",
    "    \"Empresas9.zip\",\n",
    "]\n",
    "\n",
    "current_year = date.today().year\n",
    "current_month = date.today().month\n",
    "\n",
    "years_to_download = 5\n",
    "start_year = current_year - years_to_download + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73a53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função para listar diretórios existentes ---\n",
    "async def listar_diretorios_existentes():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as resp:\n",
    "            html = await resp.text()\n",
    "            return re.findall(r\"(\\d{4}-\\d{2})/\", html)  # pega só YYYY-MM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f75cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(session: aiohttp.ClientSession, url: str, sem: asyncio.Semaphore, retries=3):\n",
    "    async with sem:\n",
    "        for tentativa in range(1, retries+1):\n",
    "            try:\n",
    "                timeout = aiohttp.ClientTimeout(total=600)\n",
    "                async with session.get(url, timeout=timeout) as response:\n",
    "\n",
    "                    status = response.status\n",
    "                    if status != 200:\n",
    "                        logger.error(f\"Url: {url}, Status: {status}, Message: Status inesperado\")\n",
    "                        return None\n",
    "\n",
    "                    content_type = response.headers.get(\"Content-Type\")\n",
    "                    if content_type != \"application/zip\":\n",
    "                        logger.error(f\"Url: {url}, Status: {status}, Message: Tipo inesperado: {content_type}\")\n",
    "                        return None\n",
    "\n",
    "                    content_length = response.headers.get(\"Content-Length\")\n",
    "                    if content_length is None or int(content_length) == 0:\n",
    "                        logger.error(f\"Url: {url}, Status: {status}, Message: Tamanho indefinido ou inesperado\")\n",
    "                        return None\n",
    "\n",
    "                    filename = url.split(\"/\")[-1]\n",
    "                    path_file = f\"download/{filename}\"\n",
    "                    with open(path_file, \"wb\") as zip_file:\n",
    "                        async for chunk in response.content.iter_chunked(1024*1024):\n",
    "                            zip_file.write(chunk)\n",
    "                    logger.info(f\"Baixado: {filename}\")\n",
    "\n",
    "                    # Extrai o zip\n",
    "                    try:\n",
    "                        with zipfile.ZipFile(path_file, \"r\") as zf:\n",
    "                            zf.extractall(\"download\")\n",
    "                        logger.info(f\"Extraído: {filename}\")\n",
    "                    except zipfile.BadZipFile as bz:\n",
    "                        logger.error(f\"Url: {url}, Error: {bz}, Message: Erro ao tentar descompactar o arquivo: {filename}\")\n",
    "                    return  # sucesso\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Tentativa {tentativa}/{retries} falhou para {url}: {e}\")\n",
    "                await asyncio.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bdb9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função para download com limite de concorrência ---\n",
    "async def downloader(urls):\n",
    "    sem = asyncio.Semaphore()  # Limite de concorrência\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch(session, url, sem) for url in urls]\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb74f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função principal ---\n",
    "async def main():\n",
    "    dirs_existentes = await listar_diretorios_existentes()\n",
    "\n",
    "    urls_para_baixar = []\n",
    "    for year in range(start_year, current_year + 1):\n",
    "        last_month = current_month if year == current_year else 12\n",
    "        for month in range(1, last_month + 1):\n",
    "            dir_name = f\"{year}-{str(month).zfill(2)}\"\n",
    "            if dir_name in dirs_existentes:  # só pega meses que existem\n",
    "                for archive in archives:\n",
    "                    urls_para_baixar.append(f\"{url}{dir_name}/{archive}\")\n",
    "\n",
    "    logger.info(f\"{len(urls_para_baixar)} arquivos para baixar\")\n",
    "    await downloader(urls_para_baixar)\n",
    "\n",
    "    # --- Após baixar, filtra apenas arquivos de Empresas ---\n",
    "    pasta_base = Path(\"download\")\n",
    "    padrao_arquivo = \"Empresas\"\n",
    "    arquivos_empresas = [arq for arq in pasta_base.rglob(\"*\") if padrao_arquivo.lower() in arq.name.lower()]\n",
    "\n",
    "    logger.info(f\"Arquivos de empresas encontrados: {len(arquivos_empresas)}\")\n",
    "    dfs = []\n",
    "    for arquivo in arquivos_empresas:\n",
    "        try:\n",
    "            df = pd.read_csv(arquivo, sep=\";\", encoding=\"latin1\", low_memory=False)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao ler {arquivo}: {e}\")\n",
    "\n",
    "    if dfs:\n",
    "        df_final = pd.concat(dfs, ignore_index=True)\n",
    "        logger.info(f\"Total de linhas consolidadas: {len(df_final)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Path(\"download\").mkdir(exist_ok=True)\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7cf43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_base = Path(\"download\")\n",
    "padrao_arquivo = \"EMPRECSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista apenas CSVs\n",
    "arquivos_empresas = [arq for arq in pasta_base.rglob(\"*\") if padrao_arquivo in arq.name.upper()]\n",
    "\n",
    "logger.info(f\"Arquivos CSV de empresas encontrados: {len(arquivos_empresas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3681a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload para o MinIO\n",
    "for arquivo in arquivos_empresas:\n",
    "    caminho_relativo = arquivo.relative_to(pasta_base)\n",
    "    destino = f\"rfb/cnpj_empresas/{caminho_relativo.as_posix()}\"\n",
    "    s3_client.fput_object(\n",
    "        bucket_name=\"landing\",\n",
    "        object_name=destino,\n",
    "        file_path=str(arquivo)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza da pasta download após upload\n",
    "downloads_path = Path(\"download\")\n",
    "try:\n",
    "    if downloads_path.exists():\n",
    "        shutil.rmtree(downloads_path)\n",
    "        logger.info(f\"Pasta '{downloads_path}' removida com sucesso após upload.\")\n",
    "    else:\n",
    "        logger.warning(f\"Pasta '{downloads_path}' não encontrada para remoção.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao tentar remover '{downloads_path}': {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto-lakehouse (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
