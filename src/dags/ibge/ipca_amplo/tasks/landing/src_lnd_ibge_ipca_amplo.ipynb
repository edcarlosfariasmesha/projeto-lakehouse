{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import requests\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7681ecd0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "minio_connection = \"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7401866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregar para funcionar\n",
    "try:\n",
    "    minio_conn = json.loads(minio_connection)\n",
    "except json.JSONDecodeError:\n",
    "    with open('../variables/minio_connection.json', \"r\") as minio_connection_file:\n",
    "        minio_conn = json.loads(minio_connection_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191d8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazySparkSession:\n",
    "    packages = [\n",
    "        \"io.delta:delta-spark_2.13:4.0.0\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0\",\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.787\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, access_key, secret_key, endpoint):\n",
    "        self._access_key = access_key\n",
    "        self._secret_key = secret_key\n",
    "        self._endpoint = endpoint\n",
    "        \n",
    "\n",
    "    def start(\n",
    "        self,\n",
    "        app_name: str = \"Airflow Spark Delta Minio App\",\n",
    "        executor_memory: str = \"1g\",\n",
    "        driver_memory: str = \"1g\",\n",
    "        driver_maxresultsize: str = \"1g\",\n",
    "        master_url: str = \"local[*]\",\n",
    "    ):\n",
    "\n",
    "        builder = (\n",
    "            SparkSession\n",
    "            .Builder()\n",
    "            .appName(app_name)\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", self._access_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", self._secret_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", self._endpoint)\n",
    "            .config(\"spark.hadoop.delta.enableFastS3AListFrom\", \"true\")\n",
    "            #\n",
    "            .config(\"spark.executor.memory\", executor_memory)\n",
    "            .config(\"spark.driver.memory\", driver_memory)\n",
    "            .config(\"spark.driver.maxResultSize\", driver_maxresultsize)\n",
    "            #\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            #\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            #\n",
    "            .config(\"spark.jars.packages\", \",\".join(self.packages))\n",
    "            .master(master_url)\n",
    "        )\n",
    "\n",
    "        return builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b73640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/28 16:07:29 WARN Utils: Your hostname, DESKTOP-EDEM2DH, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/07/28 16:07:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/edcarlos/projeto-lakehouse/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/edcarlos/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/edcarlos/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-de53d508-e17e-447d-9496-fde6253e1bdc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.4.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.23.19 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.787 in central\n",
      ":: resolution report :: resolve 301ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.787 from central in [default]\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.4.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.23.19 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-de53d508-e17e-447d-9496-fde6253e1bdc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/8ms)\n",
      "25/07/28 16:07:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = LazySparkSession(\n",
    "    access_key=minio_conn.get(\"access_key\"), \n",
    "    secret_key=minio_conn.get(\"key\"), \n",
    "    endpoint=minio_conn.get(\"endpoint\")\n",
    ").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4def379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração básica de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6565e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ibge_ipca_amplo_data(url: str) -> List[Dict[str, Any]] | None:\n",
    "    \"\"\"\n",
    "    Realiza a requisição HTTP e valida a resposta, com tratamento de erros e logs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Tentando requisição para: {url}\")\n",
    "        response = requests.get(url) \n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logging.error(f\"Erro de conexão ao acessar {url}. Verifique sua rede.\")\n",
    "        return None\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logging.error(f\"Erro HTTP na requisição para {url}: {e}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Erro inesperado na requisição para {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    content_type = response.headers.get('Content-Type', '')\n",
    "    if 'application/json' not in content_type:\n",
    "        logging.error(f\" Resposta não é JSON: {content_type}\")\n",
    "        return None\n",
    "        \n",
    "    if len(response.content) == 0:\n",
    "        logging.error(\" Resposta vazia\")\n",
    "        return None\n",
    "        \n",
    "    logging.info(\"Requisição OK.\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7050413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros fixos\n",
    "agregados = 7060\n",
    "variaveis = \"63|69|2265|66\"\n",
    "localidades = \"N1[all]|N6[all]\"\n",
    "classificacao = \"315[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b269dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar períodos do tipo \"YYYYMM\" de 2020 até hoje\n",
    "start = datetime(2020, 1, 1)\n",
    "end = datetime.today()\n",
    "periodos = [\n",
    "    (start + relativedelta(months=i)).strftime(\"%Y%m\")\n",
    "    for i in range((end.year - start.year) * 12 + end.month - start.month + 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9869da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para montar a URL\n",
    "def montar_url(periodo):\n",
    "    return (\n",
    "        f\"https://servicodados.ibge.gov.br/api/v3/agregados/{agregados}\"\n",
    "        f\"/periodos/{periodo}\"\n",
    "        f\"/variaveis/{variaveis}\"\n",
    "        f\"?localidades={localidades}\"\n",
    "        f\"&classificacao={classificacao}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5fd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coleta os dados\n",
    "dados = []\n",
    "\n",
    "for periodo in periodos:\n",
    "    url = montar_url(periodo)\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        if resp.status_code == 200 and resp.json():\n",
    "            dados.append(resp.json())\n",
    "    except Exception:\n",
    "        pass  # erro ignorado\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # Resultado: lista de JSONs por período\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5b30a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junta todos os resultados em um único response\n",
    "response = []\n",
    "\n",
    "for parcial in dados:\n",
    "    if isinstance(parcial, list):\n",
    "        response.extend(parcial)  # adiciona cada dicionário da lista parcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d61add43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if response:\n",
    "    total_registros = len(response)\n",
    "    exemplo = []\n",
    "\n",
    "    for i, item in enumerate(response[:1], start=1): \n",
    "        registro = {k: v for k, v in item.items()}\n",
    "        exemplo.append(registro)\n",
    "else:\n",
    "    total_registros = 0\n",
    "    exemplo = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15bcfdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Navega por um JSON estruturado em: resultados → classificacoes + series\n",
    "\n",
    "Faz joins manuais entre:\n",
    "\n",
    "classificações ↔ categorias\n",
    "\n",
    "localidades ↔ níveis\n",
    "\n",
    "séries ↔ períodos e valores\n",
    "\"\"\"\n",
    "\n",
    "def safe_float(valor):\n",
    "    try:\n",
    "        return float(valor)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "registros_extraidos = []\n",
    "\n",
    "for resposta in response:\n",
    "    for resultado in resposta['resultados']:\n",
    "        classificacoes = resultado['classificacoes']\n",
    "        series_temporais = resultado['series']\n",
    "\n",
    "# Para cada classificação, iteramos todas as séries temporais\n",
    "# para gerar todas as combinações possíveis (produto cartesiano)\n",
    "\n",
    "        for classificacao in classificacoes:\n",
    "            classificacao_copia = classificacao.copy()\n",
    "            categorias = classificacao_copia.pop('categoria')\n",
    "\n",
    "            for categoria_id, categoria_nome in categorias.items():\n",
    "                classificacao_id = classificacao_copia['id']\n",
    "                classificacao_nome = classificacao_copia['nome']\n",
    "\n",
    "                for serie_temporal in series_temporais:\n",
    "                    localidade = serie_temporal['localidade']\n",
    "                    serie = serie_temporal['serie']\n",
    "\n",
    "                    localidade_copia = localidade.copy()\n",
    "                    nivel_info = localidade_copia.pop('nivel')\n",
    "\n",
    "                    localidade_detalhada = {\n",
    "                        **localidade_copia,\n",
    "                        'nivel_id': nivel_info['id'],\n",
    "                        'nivel_nome': nivel_info['nome']\n",
    "                    }\n",
    "\n",
    "                    for periodo, valor in serie.items():\n",
    "                        registro = {\n",
    "                            \"id_variavel\": resposta[\"id\"],\n",
    "                            \"nome_variavel\": resposta[\"variavel\"],\n",
    "                            \"unidade_medida\": resposta[\"unidade\"],\n",
    "                            \"id_classificacao\": classificacao_id,\n",
    "                            \"nome_classificacao\": classificacao_nome,\n",
    "                            \"id_categoria\": categoria_id,\n",
    "                            \"nome_categoria\": categoria_nome,\n",
    "                            \"id_localidade\": localidade_detalhada['id'],\n",
    "                            \"nome_localidade\": localidade_detalhada['nome'],\n",
    "                            \"id_nivel\": localidade_detalhada['nivel_id'],\n",
    "                            \"nome_nivel\": localidade_detalhada['nivel_nome'],\n",
    "                            \"periodo\": periodo,\n",
    "                            \"valor\": safe_float(valor)\n",
    "                        }\n",
    "                        registros_extraidos.append(registro)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3267035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(registros_extraidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd174f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/28 16:09:17 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "25/07/28 16:09:21 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/07/28 16:09:27 WARN TaskSetManager: Stage 6 contains a task of very large size (9086 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/28 16:09:30 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/07/28 16:09:32 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to ibge/ipca_amplo/part-00005-ae7b59cd-301f-4a4f-9b51-c25d6201ace4-c000.snappy.parquet. This is Unsupported\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://landing/ibge/ipca_amplo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fdf1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+-------------+--------+-----------+--------------------+--------------------+-----------------+----------+--------------------+-------+--------------+-----+\n",
      "|id_categoria|id_classificacao|id_localidade|id_nivel|id_variavel|      nome_categoria|  nome_classificacao|  nome_localidade|nome_nivel|       nome_variavel|periodo|unidade_medida|valor|\n",
      "+------------+----------------+-------------+--------+-----------+--------------------+--------------------+-----------------+----------+--------------------+-------+--------------+-----+\n",
      "|      107674|             315|      5300108|      N6|         69|8101006.Pós-gradu...|Geral, grupo, sub...|    Brasília - DF| Município|IPCA - Variação a...| 202105|             %| 0.53|\n",
      "|       47664|             315|            1|      N1|         69|8101008.Educação ...|Geral, grupo, sub...|           Brasil|    Brasil|IPCA - Variação a...| 202105|             %|  1.1|\n",
      "|       47664|             315|      1200401|      N6|         69|8101008.Educação ...|Geral, grupo, sub...|  Rio Branco - AC| Município|IPCA - Variação a...| 202105|             %| 1.09|\n",
      "|       47664|             315|      2111300|      N6|         69|8101008.Educação ...|Geral, grupo, sub...|    São Luís - MA| Município|IPCA - Variação a...| 202105|             %| 1.15|\n",
      "|       47664|             315|      2800308|      N6|         69|8101008.Educação ...|Geral, grupo, sub...|     Aracaju - SE| Município|IPCA - Variação a...| 202105|             %| NULL|\n",
      "|       47664|             315|      5002704|      N6|         69|8101008.Educação ...|Geral, grupo, sub...|Campo Grande - MS| Município|IPCA - Variação a...| 202105|             %| NULL|\n",
      "|       47664|             315|      5208707|      N6|         69|8101008.Educação ...|Geral, grupo, sub...|     Goiânia - GO| Município|IPCA - Variação a...| 202105|             %| NULL|\n",
      "|       47664|             315|      5300108|      N6|         69|8101008.Educação ...|Geral, grupo, sub...|    Brasília - DF| Município|IPCA - Variação a...| 202105|             %|  0.0|\n",
      "|       47665|             315|            1|      N1|         69|8101045.Curso téc...|Geral, grupo, sub...|           Brasil|    Brasil|IPCA - Variação a...| 202105|             %| 1.12|\n",
      "|       47665|             315|      1200401|      N6|         69|8101045.Curso téc...|Geral, grupo, sub...|  Rio Branco - AC| Município|IPCA - Variação a...| 202105|             %| NULL|\n",
      "|       47665|             315|      2111300|      N6|         69|8101045.Curso téc...|Geral, grupo, sub...|    São Luís - MA| Município|IPCA - Variação a...| 202105|             %| 0.99|\n",
      "|       47665|             315|      2800308|      N6|         69|8101045.Curso téc...|Geral, grupo, sub...|     Aracaju - SE| Município|IPCA - Variação a...| 202105|             %| NULL|\n",
      "|       47665|             315|      5002704|      N6|         69|8101045.Curso téc...|Geral, grupo, sub...|Campo Grande - MS| Município|IPCA - Variação a...| 202105|             %| NULL|\n",
      "|       47665|             315|      5208707|      N6|         69|8101045.Curso téc...|Geral, grupo, sub...|     Goiânia - GO| Município|IPCA - Variação a...| 202105|             %| NULL|\n",
      "|       47665|             315|      5300108|      N6|         69|8101045.Curso téc...|Geral, grupo, sub...|    Brasília - DF| Município|IPCA - Variação a...| 202105|             %| NULL|\n",
      "|        7777|             315|            1|      N1|         69|        8102.Leitura|Geral, grupo, sub...|           Brasil|    Brasil|IPCA - Variação a...| 202105|             %|  1.3|\n",
      "|        7777|             315|      1200401|      N6|         69|        8102.Leitura|Geral, grupo, sub...|  Rio Branco - AC| Município|IPCA - Variação a...| 202105|             %| 2.85|\n",
      "|        7777|             315|      2111300|      N6|         69|        8102.Leitura|Geral, grupo, sub...|    São Luís - MA| Município|IPCA - Variação a...| 202105|             %| 0.28|\n",
      "|        7777|             315|      2800308|      N6|         69|        8102.Leitura|Geral, grupo, sub...|     Aracaju - SE| Município|IPCA - Variação a...| 202105|             %| 1.95|\n",
      "|        7777|             315|      5002704|      N6|         69|        8102.Leitura|Geral, grupo, sub...|Campo Grande - MS| Município|IPCA - Variação a...| 202105|             %| 0.97|\n",
      "+------------+----------------+-------------+--------+-----------+--------------------+--------------------+-----------------+----------+--------------------+-------+--------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto-lakehouse (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
